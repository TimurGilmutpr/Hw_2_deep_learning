{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc450fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "images = np.load('images.npy')\n",
    "labels = np.load('labels.npy')\n",
    "images_sub = np.load('images_sub.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3e9472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         ...,\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3]],\n",
       "\n",
       "        [[ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         ...,\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144]],\n",
       "\n",
       "        [[ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         ...,\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         ...,\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144]],\n",
       "\n",
       "        [[ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         ...,\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3],\n",
       "         [ 76,   5,   3]],\n",
       "\n",
       "        [[ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         ...,\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144],\n",
       "         [ 20, 172, 144]]],\n",
       "\n",
       "\n",
       "       [[[152, 124, 239],\n",
       "         [154, 126, 236],\n",
       "         [156, 128, 213],\n",
       "         ...,\n",
       "         [ 96,  64, 239],\n",
       "         [106,  74, 243],\n",
       "         [ 90,  58, 235]],\n",
       "\n",
       "        [[ 97,  66, 224],\n",
       "         [124,  95, 231],\n",
       "         [136, 108, 199],\n",
       "         ...,\n",
       "         [112,  82, 234],\n",
       "         [103,  71, 240],\n",
       "         [ 89,  57, 238]],\n",
       "\n",
       "        [[104,  72, 247],\n",
       "         [121,  90, 243],\n",
       "         [138, 110, 213],\n",
       "         ...,\n",
       "         [128,  99, 207],\n",
       "         [103,  72, 221],\n",
       "         [ 84,  53, 228]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[100, 144,  94],\n",
       "         [ 94, 138,  88],\n",
       "         [ 96, 138,  88],\n",
       "         ...,\n",
       "         [124, 160, 120],\n",
       "         [108, 148, 102],\n",
       "         [102, 144,  96]],\n",
       "\n",
       "        [[100, 144,  94],\n",
       "         [ 94, 138,  88],\n",
       "         [ 92, 136,  86],\n",
       "         ...,\n",
       "         [126, 160, 120],\n",
       "         [108, 148, 100],\n",
       "         [100, 142,  94]],\n",
       "\n",
       "        [[ 98, 142,  92],\n",
       "         [ 94, 138,  88],\n",
       "         [ 94, 136,  86],\n",
       "         ...,\n",
       "         [130, 166, 124],\n",
       "         [112, 154, 106],\n",
       "         [104, 148,  98]]],\n",
       "\n",
       "\n",
       "       [[[191,  14, 239],\n",
       "         [197, 185, 132],\n",
       "         [191,  14, 239],\n",
       "         ...,\n",
       "         [149, 191, 120],\n",
       "         [191,  14, 239],\n",
       "         [154, 191, 121]],\n",
       "\n",
       "        [[191,  14, 239],\n",
       "         [197, 185, 132],\n",
       "         [191,  14, 239],\n",
       "         ...,\n",
       "         [149, 191, 120],\n",
       "         [191,  14, 239],\n",
       "         [156, 190, 122]],\n",
       "\n",
       "        [[191,  14, 239],\n",
       "         [197, 185, 132],\n",
       "         [191,  14, 239],\n",
       "         ...,\n",
       "         [149, 191, 120],\n",
       "         [191,  14, 239],\n",
       "         [156, 190, 122]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[191,  14, 239],\n",
       "         [197, 185, 132],\n",
       "         [191,  14, 239],\n",
       "         ...,\n",
       "         [149, 191, 120],\n",
       "         [191,  14, 239],\n",
       "         [149, 191, 120]],\n",
       "\n",
       "        [[191,  14, 239],\n",
       "         [197, 185, 132],\n",
       "         [191,  14, 239],\n",
       "         ...,\n",
       "         [149, 191, 120],\n",
       "         [191,  14, 239],\n",
       "         [149, 191, 120]],\n",
       "\n",
       "        [[191,  14, 239],\n",
       "         [197, 185, 132],\n",
       "         [191,  14, 239],\n",
       "         ...,\n",
       "         [149, 191, 120],\n",
       "         [191,  14, 239],\n",
       "         [149, 191, 120]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[113, 167, 145],\n",
       "         [113, 166, 144],\n",
       "         [116, 167, 143],\n",
       "         ...,\n",
       "         [ 23, 200,  94],\n",
       "         [ 25, 200,  94],\n",
       "         [ 26, 195,  94]],\n",
       "\n",
       "        [[113, 165, 146],\n",
       "         [113, 165, 145],\n",
       "         [114, 165, 143],\n",
       "         ...,\n",
       "         [ 43, 192, 104],\n",
       "         [ 34, 196,  99],\n",
       "         [ 31, 196,  97]],\n",
       "\n",
       "        [[114, 166, 145],\n",
       "         [113, 164, 144],\n",
       "         [112, 164, 144],\n",
       "         ...,\n",
       "         [ 87, 174, 128],\n",
       "         [ 74, 183, 117],\n",
       "         [ 64, 186, 113]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[141, 185, 146],\n",
       "         [143, 185, 150],\n",
       "         [144, 184, 151],\n",
       "         ...,\n",
       "         [137, 181, 149],\n",
       "         [138, 186, 148],\n",
       "         [145, 187, 146]],\n",
       "\n",
       "        [[147, 187, 141],\n",
       "         [146, 186, 145],\n",
       "         [138, 186, 143],\n",
       "         ...,\n",
       "         [138, 182, 150],\n",
       "         [139, 184, 150],\n",
       "         [144, 183, 148]],\n",
       "\n",
       "        [[147, 190, 137],\n",
       "         [145, 187, 138],\n",
       "         [134, 192, 135],\n",
       "         ...,\n",
       "         [143, 181, 148],\n",
       "         [146, 175, 149],\n",
       "         [143, 172, 145]]],\n",
       "\n",
       "\n",
       "       [[[208, 198, 210],\n",
       "         [214, 197, 209],\n",
       "         [213, 198, 209],\n",
       "         ...,\n",
       "         [207, 202, 207],\n",
       "         [208, 200, 204],\n",
       "         [204, 196, 204]],\n",
       "\n",
       "        [[209, 200, 205],\n",
       "         [210, 199, 209],\n",
       "         [207, 200, 209],\n",
       "         ...,\n",
       "         [207, 201, 203],\n",
       "         [208, 203, 204],\n",
       "         [210, 201, 207]],\n",
       "\n",
       "        [[210, 204, 206],\n",
       "         [207, 203, 212],\n",
       "         [204, 205, 210],\n",
       "         ...,\n",
       "         [205, 198, 205],\n",
       "         [207, 203, 209],\n",
       "         [210, 205, 210]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 96, 121, 118],\n",
       "         [ 99, 125, 121],\n",
       "         [ 95, 122, 117],\n",
       "         ...,\n",
       "         [ 45,  68,  57],\n",
       "         [ 49,  72,  62],\n",
       "         [ 50,  73,  64]],\n",
       "\n",
       "        [[116, 141, 138],\n",
       "         [115, 142, 139],\n",
       "         [115, 142, 137],\n",
       "         ...,\n",
       "         [ 44,  66,  56],\n",
       "         [ 49,  70,  60],\n",
       "         [ 52,  73,  64]],\n",
       "\n",
       "        [[117, 140, 139],\n",
       "         [116, 140, 139],\n",
       "         [115, 140, 136],\n",
       "         ...,\n",
       "         [ 45,  67,  57],\n",
       "         [ 48,  70,  60],\n",
       "         [ 51,  73,  64]]],\n",
       "\n",
       "\n",
       "       [[[ 55, 124,  77],\n",
       "         [ 50, 123,  78],\n",
       "         [ 56, 134,  82],\n",
       "         ...,\n",
       "         [109,  73,  56],\n",
       "         [111,  75,  59],\n",
       "         [108,  73,  56]],\n",
       "\n",
       "        [[ 49, 122,  78],\n",
       "         [ 49, 124,  79],\n",
       "         [ 60, 139,  84],\n",
       "         ...,\n",
       "         [108,  72,  56],\n",
       "         [108,  73,  56],\n",
       "         [107,  72,  55]],\n",
       "\n",
       "        [[ 48, 123,  79],\n",
       "         [ 51, 128,  80],\n",
       "         [ 65, 148,  86],\n",
       "         ...,\n",
       "         [109,  74,  57],\n",
       "         [109,  74,  57],\n",
       "         [106,  71,  54]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 61, 113, 153],\n",
       "         [ 95, 138, 143],\n",
       "         [151, 176, 135],\n",
       "         ...,\n",
       "         [ 81, 191,  58],\n",
       "         [ 94, 203,  54],\n",
       "         [ 99, 208,  50]],\n",
       "\n",
       "        [[ 59, 113, 161],\n",
       "         [ 69, 118, 160],\n",
       "         [ 94, 129, 165],\n",
       "         ...,\n",
       "         [ 81, 131, 121],\n",
       "         [ 92, 163,  97],\n",
       "         [ 93, 179,  79]],\n",
       "\n",
       "        [[ 61, 115, 163],\n",
       "         [ 62, 114, 163],\n",
       "         [ 70, 113, 170],\n",
       "         ...,\n",
       "         [ 91, 100, 147],\n",
       "         [ 97, 121, 140],\n",
       "         [ 90, 126, 138]]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6c7ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[144, 109, 189],\n",
       "         [153, 122, 179],\n",
       "         [169, 143, 163],\n",
       "         ...,\n",
       "         [158, 130, 175],\n",
       "         [149, 117, 183],\n",
       "         [131,  91, 200]],\n",
       "\n",
       "        [[128,  87, 203],\n",
       "         [138, 101, 193],\n",
       "         [160, 132, 172],\n",
       "         ...,\n",
       "         [158, 129, 175],\n",
       "         [152, 121, 181],\n",
       "         [132,  93, 199]],\n",
       "\n",
       "        [[127,  86, 204],\n",
       "         [135,  96, 197],\n",
       "         [145, 111, 188],\n",
       "         ...,\n",
       "         [158, 129, 175],\n",
       "         [152, 121, 181],\n",
       "         [132,  93, 198]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[147, 115, 185],\n",
       "         [159, 131, 174],\n",
       "         [168, 142, 165],\n",
       "         ...,\n",
       "         [159, 130, 174],\n",
       "         [154, 123, 179],\n",
       "         [137, 100, 194]],\n",
       "\n",
       "        [[130,  90, 201],\n",
       "         [143, 108, 189],\n",
       "         [165, 138, 168],\n",
       "         ...,\n",
       "         [159, 130, 175],\n",
       "         [153, 122, 180],\n",
       "         [131,  91, 200]],\n",
       "\n",
       "        [[130,  90, 201],\n",
       "         [138, 102, 194],\n",
       "         [139, 102, 193],\n",
       "         ...,\n",
       "         [158, 129, 175],\n",
       "         [148, 116, 184],\n",
       "         [128,  87, 203]]],\n",
       "\n",
       "\n",
       "       [[[ 89,  64, 132],\n",
       "         [ 90,  61, 132],\n",
       "         [ 94,  60, 133],\n",
       "         ...,\n",
       "         [100,  63, 136],\n",
       "         [ 97,  60, 133],\n",
       "         [101,  62, 133]],\n",
       "\n",
       "        [[ 89,  63, 131],\n",
       "         [ 92,  60, 130],\n",
       "         [ 94,  61, 133],\n",
       "         ...,\n",
       "         [ 98,  66, 137],\n",
       "         [ 96,  61, 133],\n",
       "         [ 99,  60, 133]],\n",
       "\n",
       "        [[ 94,  62, 130],\n",
       "         [ 94,  61, 132],\n",
       "         [ 94,  64, 135],\n",
       "         ...,\n",
       "         [ 97,  70, 137],\n",
       "         [ 94,  65, 132],\n",
       "         [ 96,  59, 133]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[252, 194, 225],\n",
       "         [251, 193, 224],\n",
       "         [251, 192, 222],\n",
       "         ...,\n",
       "         [248, 188, 222],\n",
       "         [247, 189, 221],\n",
       "         [246, 190, 221]],\n",
       "\n",
       "        [[252, 192, 228],\n",
       "         [252, 192, 226],\n",
       "         [253, 191, 224],\n",
       "         ...,\n",
       "         [251, 190, 223],\n",
       "         [251, 190, 223],\n",
       "         [252, 192, 223]],\n",
       "\n",
       "        [[251, 191, 228],\n",
       "         [252, 191, 227],\n",
       "         [252, 191, 224],\n",
       "         ...,\n",
       "         [251, 191, 224],\n",
       "         [251, 191, 223],\n",
       "         [252, 192, 223]]],\n",
       "\n",
       "\n",
       "       [[[178, 195,  45],\n",
       "         [185, 203,  26],\n",
       "         [187, 207,  15],\n",
       "         ...,\n",
       "         [187, 207,  11],\n",
       "         [188, 205,  10],\n",
       "         [188, 206,  11]],\n",
       "\n",
       "        [[177, 194,  50],\n",
       "         [183, 204,  29],\n",
       "         [187, 207,  16],\n",
       "         ...,\n",
       "         [187, 207,  11],\n",
       "         [188, 206,  11],\n",
       "         [187, 207,  10]],\n",
       "\n",
       "        [[174, 194,  54],\n",
       "         [182, 204,  31],\n",
       "         [187, 207,  17],\n",
       "         ...,\n",
       "         [187, 207,  12],\n",
       "         [187, 207,  10],\n",
       "         [187, 208,   9]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[179, 198,  37],\n",
       "         [182, 206,  23],\n",
       "         [188, 209,  13],\n",
       "         ...,\n",
       "         [187, 207,  12],\n",
       "         [185, 207,  12],\n",
       "         [186, 206,  11]],\n",
       "\n",
       "        [[176, 196,  45],\n",
       "         [184, 206,  24],\n",
       "         [187, 208,  13],\n",
       "         ...,\n",
       "         [187, 207,  11],\n",
       "         [185, 207,  11],\n",
       "         [183, 208,  13]],\n",
       "\n",
       "        [[179, 200,  39],\n",
       "         [185, 207,  19],\n",
       "         [187, 207,  13],\n",
       "         ...,\n",
       "         [188, 208,  11],\n",
       "         [183, 209,  13],\n",
       "         [184, 208,  12]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[  2,  69, 235],\n",
       "         [210, 123, 156],\n",
       "         [  2,  69, 235],\n",
       "         ...,\n",
       "         [209, 123, 155],\n",
       "         [  2,  69, 235],\n",
       "         [220, 130, 165]],\n",
       "\n",
       "        [[  2,  69, 235],\n",
       "         [209, 123, 155],\n",
       "         [  2,  69, 235],\n",
       "         ...,\n",
       "         [209, 122, 154],\n",
       "         [  2,  69, 235],\n",
       "         [216, 128, 162]],\n",
       "\n",
       "        [[  2,  69, 235],\n",
       "         [209, 123, 155],\n",
       "         [  2,  69, 235],\n",
       "         ...,\n",
       "         [209, 122, 154],\n",
       "         [  2,  69, 235],\n",
       "         [213, 125, 159]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  2,  69, 235],\n",
       "         [213, 125, 159],\n",
       "         [  2,  69, 235],\n",
       "         ...,\n",
       "         [209, 123, 155],\n",
       "         [  2,  69, 235],\n",
       "         [223, 133, 169]],\n",
       "\n",
       "        [[  2,  69, 235],\n",
       "         [221, 131, 167],\n",
       "         [  2,  69, 235],\n",
       "         ...,\n",
       "         [209, 123, 155],\n",
       "         [  2,  69, 235],\n",
       "         [222, 132, 168]],\n",
       "\n",
       "        [[  2,  69, 235],\n",
       "         [226, 135, 172],\n",
       "         [  2,  69, 235],\n",
       "         ...,\n",
       "         [209, 123, 155],\n",
       "         [  2,  69, 235],\n",
       "         [220, 131, 166]]],\n",
       "\n",
       "\n",
       "       [[[228,  44, 216],\n",
       "         [217,  23, 193],\n",
       "         [204,  18, 174],\n",
       "         ...,\n",
       "         [108,  75,  77],\n",
       "         [114,  79,  82],\n",
       "         [ 96,  66,  71]],\n",
       "\n",
       "        [[199,  21, 175],\n",
       "         [234,  49, 222],\n",
       "         [238,  57, 222],\n",
       "         ...,\n",
       "         [ 65,  37,  44],\n",
       "         [ 79,  43,  50],\n",
       "         [ 80,  37,  57]],\n",
       "\n",
       "        [[226,  25, 196],\n",
       "         [247,  56, 234],\n",
       "         [244,  39, 221],\n",
       "         ...,\n",
       "         [115,  31,  84],\n",
       "         [105,  35,  76],\n",
       "         [113,  40, 100]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[226,  27, 204],\n",
       "         [203,   8, 170],\n",
       "         [191,  12, 159],\n",
       "         ...,\n",
       "         [249,  35, 221],\n",
       "         [228,  36, 207],\n",
       "         [163,  19, 184]],\n",
       "\n",
       "        [[196,  15, 167],\n",
       "         [212,   8, 179],\n",
       "         [217,  15, 186],\n",
       "         ...,\n",
       "         [246,  39, 213],\n",
       "         [212,  27, 189],\n",
       "         [150,   9, 169]],\n",
       "\n",
       "        [[232,  29, 209],\n",
       "         [233,  20, 206],\n",
       "         [228,  43, 214],\n",
       "         ...,\n",
       "         [252,  57, 225],\n",
       "         [229,  47, 215],\n",
       "         [174,  21, 192]]],\n",
       "\n",
       "\n",
       "       [[[100, 199, 222],\n",
       "         [ 99, 200, 220],\n",
       "         [ 94, 204, 223],\n",
       "         ...,\n",
       "         [ 88, 204, 225],\n",
       "         [ 93, 204, 224],\n",
       "         [ 92, 202, 228]],\n",
       "\n",
       "        [[103, 200, 224],\n",
       "         [ 97, 203, 222],\n",
       "         [ 91, 203, 226],\n",
       "         ...,\n",
       "         [ 92, 204, 223],\n",
       "         [ 93, 202, 226],\n",
       "         [ 94, 203, 226]],\n",
       "\n",
       "        [[100, 201, 225],\n",
       "         [ 95, 202, 222],\n",
       "         [ 92, 202, 222],\n",
       "         ...,\n",
       "         [ 91, 203, 226],\n",
       "         [ 90, 203, 227],\n",
       "         [ 91, 203, 224]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[185, 208, 205],\n",
       "         [185, 208, 205],\n",
       "         [185, 208, 205],\n",
       "         ...,\n",
       "         [ 95, 199, 227],\n",
       "         [ 96, 200, 229],\n",
       "         [ 97, 200, 230]],\n",
       "\n",
       "        [[185, 208, 205],\n",
       "         [185, 208, 205],\n",
       "         [185, 208, 205],\n",
       "         ...,\n",
       "         [133, 204, 215],\n",
       "         [ 98, 199, 225],\n",
       "         [ 92, 199, 224]],\n",
       "\n",
       "        [[185, 208, 205],\n",
       "         [185, 208, 205],\n",
       "         [185, 208, 205],\n",
       "         ...,\n",
       "         [185, 208, 205],\n",
       "         [171, 206, 207],\n",
       "         [131, 202, 215]]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a093bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2f045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "uniq_labels = set(labels)\n",
    "print(uniq_labels)\n",
    "print(len(uniq_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 17:26:36.174789: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-31 17:26:36.186584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748701596.198686   40379 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748701596.202182   40379 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748701596.212203   40379 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748701596.212231   40379 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748701596.212233   40379 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748701596.212235   40379 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-31 17:26:36.216262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "#нормализуем данные\n",
    "X = images.astype('float32') / 255.0\n",
    "y = to_categorical(labels, num_classes=26)\n",
    "X_sub = images_sub.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5565bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#разделим на тестовую обучающую и валидационную\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.15, random_state=42, stratify=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3809114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpus available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models \n",
    "print(\"Gpus available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#какие гпу у меня есть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU details: {'compute_capability': (8, 9), 'device_name': 'NVIDIA GeForce RTX 4070 Laptop GPU'}\n"
     ]
    }
   ],
   "source": [
    "for gpu in gpus:\n",
    "    details = tf.config.experimental.get_device_details(gpu)\n",
    "    print(\"GPU details:\", details)\n",
    "# обучу все на ноутбучой гпу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7931c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 31 17:43:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.51.03              Driver Version: 576.28         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   42C    P8              2W /  140W |    6104MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           40379      C   /python3.9                            N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408befaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsl/miniconda3/envs/myenv/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(26, activation='softmax')\n",
    "])#простая модель и переобучается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48646f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsl/miniconda3/envs/myenv/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1748701603.716682   40379 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(48,48,3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.35),\n",
    "\n",
    "    layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Dropout(0.45),\n",
    "\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(26, activation='softmax')\n",
    "])#больше слоев!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#компилирем с adam и acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9866eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#сделаем аугментации\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c826cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b269d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 17:28:12.909846: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3014', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.272574: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3014_0', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.273802: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3014_0', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.399655: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3616', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.690116: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3616', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.706573: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3692', 116 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.732006: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3014', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.872856: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3616', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:13.996306: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3692_0', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:14.789829: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4251', 576 bytes spill stores, 1180 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:14.917690: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3692', 220 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:14.926104: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4251', 560 bytes spill stores, 1336 bytes spill loads\n",
      "\n",
      "2025-05-31 17:28:12.738299: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4251', 564 bytes spill stores, 1188 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 50ms/step - accuracy: 0.0461 - loss: 3.7706 - val_accuracy: 0.0293 - val_loss: 3.5444\n",
      "Epoch 2/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0703 - loss: 3.5559 - val_accuracy: 0.0280 - val_loss: 3.5446\n",
      "Epoch 3/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.0457 - loss: 3.5213 - val_accuracy: 0.0247 - val_loss: 3.4046\n",
      "Epoch 4/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0859 - loss: 3.3578 - val_accuracy: 0.0243 - val_loss: 3.4040\n",
      "Epoch 5/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.0509 - loss: 3.4218 - val_accuracy: 0.0667 - val_loss: 3.2089\n",
      "Epoch 6/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0781 - loss: 3.4521 - val_accuracy: 0.0693 - val_loss: 3.2147\n",
      "Epoch 7/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.0711 - loss: 3.3125 - val_accuracy: 0.0867 - val_loss: 3.0685\n",
      "Epoch 8/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1016 - loss: 3.3686 - val_accuracy: 0.0903 - val_loss: 3.0632\n",
      "Epoch 9/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.1128 - loss: 3.0873 - val_accuracy: 0.1247 - val_loss: 2.9796\n",
      "Epoch 10/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1719 - loss: 2.8363 - val_accuracy: 0.1307 - val_loss: 2.9807\n",
      "Epoch 11/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.1947 - loss: 2.7268 - val_accuracy: 0.1980 - val_loss: 2.7815\n",
      "Epoch 12/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3203 - loss: 2.3290 - val_accuracy: 0.1970 - val_loss: 2.7634\n",
      "Epoch 13/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.3393 - loss: 2.1561 - val_accuracy: 0.4477 - val_loss: 1.7975\n",
      "Epoch 14/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5469 - loss: 1.5153 - val_accuracy: 0.4420 - val_loss: 1.8051\n",
      "Epoch 15/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.4811 - loss: 1.6860 - val_accuracy: 0.5860 - val_loss: 1.3494\n",
      "Epoch 16/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5703 - loss: 1.5010 - val_accuracy: 0.5873 - val_loss: 1.3428\n",
      "Epoch 17/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.5727 - loss: 1.3683 - val_accuracy: 0.7097 - val_loss: 0.9446\n",
      "Epoch 18/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6797 - loss: 1.0329 - val_accuracy: 0.7027 - val_loss: 0.9514\n",
      "Epoch 19/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.6612 - loss: 1.0399 - val_accuracy: 0.7893 - val_loss: 0.6737\n",
      "Epoch 20/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6797 - loss: 0.9573 - val_accuracy: 0.7973 - val_loss: 0.6613\n",
      "Epoch 21/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.7259 - loss: 0.8396 - val_accuracy: 0.7967 - val_loss: 0.6559\n",
      "Epoch 22/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7266 - loss: 0.8466 - val_accuracy: 0.8030 - val_loss: 0.6537\n",
      "Epoch 23/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.7663 - loss: 0.7446 - val_accuracy: 0.8070 - val_loss: 0.6070\n",
      "Epoch 24/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.6394 - val_accuracy: 0.8047 - val_loss: 0.6112\n",
      "Epoch 25/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.7966 - loss: 0.6496 - val_accuracy: 0.8527 - val_loss: 0.4718\n",
      "Epoch 26/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7969 - loss: 0.6901 - val_accuracy: 0.8580 - val_loss: 0.4705\n",
      "Epoch 27/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.8182 - loss: 0.5698 - val_accuracy: 0.8623 - val_loss: 0.4369\n",
      "Epoch 28/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7969 - loss: 0.5761 - val_accuracy: 0.8653 - val_loss: 0.4310\n",
      "Epoch 29/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8259 - loss: 0.5387 - val_accuracy: 0.8117 - val_loss: 0.6536\n",
      "Epoch 30/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.4841 - val_accuracy: 0.8260 - val_loss: 0.5875\n",
      "Epoch 31/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8411 - loss: 0.4977 - val_accuracy: 0.8737 - val_loss: 0.3982\n",
      "Epoch 32/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.4628 - val_accuracy: 0.8693 - val_loss: 0.3993\n",
      "Epoch 33/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.8560 - loss: 0.4494 - val_accuracy: 0.8890 - val_loss: 0.3487\n",
      "Epoch 34/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8984 - loss: 0.3967 - val_accuracy: 0.8900 - val_loss: 0.3458\n",
      "Epoch 35/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.8643 - loss: 0.4263 - val_accuracy: 0.8753 - val_loss: 0.3972\n",
      "Epoch 36/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8359 - loss: 0.5118 - val_accuracy: 0.8753 - val_loss: 0.3897\n",
      "Epoch 37/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8614 - loss: 0.4196 - val_accuracy: 0.8980 - val_loss: 0.3293\n",
      "Epoch 38/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8906 - loss: 0.3789 - val_accuracy: 0.8970 - val_loss: 0.3363\n",
      "Epoch 39/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.8750 - loss: 0.3953 - val_accuracy: 0.8730 - val_loss: 0.4274\n",
      "Epoch 40/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.5050 - val_accuracy: 0.8647 - val_loss: 0.4521\n",
      "Epoch 41/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8783 - loss: 0.3766 - val_accuracy: 0.9110 - val_loss: 0.2971\n",
      "Epoch 42/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8906 - loss: 0.4101 - val_accuracy: 0.9087 - val_loss: 0.2991\n",
      "Epoch 43/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8897 - loss: 0.3522 - val_accuracy: 0.9083 - val_loss: 0.2985\n",
      "Epoch 44/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8672 - loss: 0.4083 - val_accuracy: 0.9067 - val_loss: 0.2940\n",
      "Epoch 45/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.8893 - loss: 0.3456 - val_accuracy: 0.8927 - val_loss: 0.3422\n",
      "Epoch 46/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.3319 - val_accuracy: 0.8943 - val_loss: 0.3385\n",
      "Epoch 47/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8947 - loss: 0.3223 - val_accuracy: 0.9110 - val_loss: 0.2947\n",
      "Epoch 48/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9297 - loss: 0.2423 - val_accuracy: 0.9110 - val_loss: 0.2934\n",
      "Epoch 49/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8910 - loss: 0.3279 - val_accuracy: 0.9100 - val_loss: 0.2808\n",
      "Epoch 50/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.2697 - val_accuracy: 0.9093 - val_loss: 0.2795\n",
      "Epoch 51/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9038 - loss: 0.3042 - val_accuracy: 0.9087 - val_loss: 0.2954\n",
      "Epoch 52/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9219 - loss: 0.2038 - val_accuracy: 0.9057 - val_loss: 0.3028\n",
      "Epoch 53/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.9009 - loss: 0.3072 - val_accuracy: 0.9113 - val_loss: 0.2855\n",
      "Epoch 54/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.3610 - val_accuracy: 0.9103 - val_loss: 0.2896\n",
      "Epoch 55/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9042 - loss: 0.2961 - val_accuracy: 0.9140 - val_loss: 0.2621\n",
      "Epoch 56/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8828 - loss: 0.3075 - val_accuracy: 0.9113 - val_loss: 0.2632\n",
      "Epoch 57/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9080 - loss: 0.2803 - val_accuracy: 0.9257 - val_loss: 0.2361\n",
      "Epoch 58/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8906 - loss: 0.3223 - val_accuracy: 0.9270 - val_loss: 0.2334\n",
      "Epoch 59/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9075 - loss: 0.2863 - val_accuracy: 0.9273 - val_loss: 0.2419\n",
      "Epoch 60/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9297 - loss: 0.1991 - val_accuracy: 0.9250 - val_loss: 0.2458\n",
      "Epoch 61/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9195 - loss: 0.2536 - val_accuracy: 0.8993 - val_loss: 0.3277\n",
      "Epoch 62/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8906 - loss: 0.3733 - val_accuracy: 0.9030 - val_loss: 0.3222\n",
      "Epoch 63/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.9150 - loss: 0.2642 - val_accuracy: 0.9283 - val_loss: 0.2350\n",
      "Epoch 64/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9297 - loss: 0.2122 - val_accuracy: 0.9287 - val_loss: 0.2339\n",
      "Epoch 65/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9171 - loss: 0.2561 - val_accuracy: 0.9000 - val_loss: 0.3188\n",
      "Epoch 66/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1656 - val_accuracy: 0.9040 - val_loss: 0.3033\n",
      "Epoch 67/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9202 - loss: 0.2506 - val_accuracy: 0.9307 - val_loss: 0.2240\n",
      "Epoch 68/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9141 - loss: 0.2187 - val_accuracy: 0.9290 - val_loss: 0.2268\n",
      "Epoch 69/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9257 - loss: 0.2323 - val_accuracy: 0.9223 - val_loss: 0.2702\n",
      "Epoch 70/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.2249 - val_accuracy: 0.9217 - val_loss: 0.2762\n",
      "Epoch 71/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9218 - loss: 0.2316 - val_accuracy: 0.9210 - val_loss: 0.2638\n",
      "Epoch 72/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.3927 - val_accuracy: 0.9207 - val_loss: 0.2726\n",
      "Epoch 73/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.9227 - loss: 0.2325 - val_accuracy: 0.9320 - val_loss: 0.2201\n",
      "Epoch 74/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9453 - loss: 0.1857 - val_accuracy: 0.9340 - val_loss: 0.2150\n",
      "Epoch 75/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9272 - loss: 0.2269 - val_accuracy: 0.9217 - val_loss: 0.2784\n",
      "Epoch 76/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9141 - loss: 0.2375 - val_accuracy: 0.9180 - val_loss: 0.2894\n",
      "Epoch 77/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9246 - loss: 0.2318 - val_accuracy: 0.9313 - val_loss: 0.2283\n",
      "Epoch 78/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.2942 - val_accuracy: 0.9307 - val_loss: 0.2245\n",
      "Epoch 79/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9248 - loss: 0.2266 - val_accuracy: 0.9287 - val_loss: 0.2233\n",
      "Epoch 80/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9453 - loss: 0.1673 - val_accuracy: 0.9287 - val_loss: 0.2266\n",
      "Epoch 81/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.9264 - loss: 0.2218 - val_accuracy: 0.9187 - val_loss: 0.2717\n",
      "Epoch 82/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.2709 - val_accuracy: 0.9173 - val_loss: 0.2759\n",
      "Epoch 83/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9274 - loss: 0.2189 - val_accuracy: 0.9307 - val_loss: 0.2234\n",
      "Epoch 84/100\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.1372 - val_accuracy: 0.9307 - val_loss: 0.2252\n"
     ]
    }
   ],
   "source": [
    "# Обучение с аугментацией\n",
    "batch_size = 128\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1770dedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>49995</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>49996</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>49997</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>49998</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>49999</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Category\n",
       "0          0        13\n",
       "1          1        20\n",
       "2          2        12\n",
       "3          3         7\n",
       "4          4         8\n",
       "...      ...       ...\n",
       "49995  49995        15\n",
       "49996  49996         4\n",
       "49997  49997        16\n",
       "49998  49998         8\n",
       "49999  49999         4\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9731e35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 17:37:53.308407: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_201', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-05-31 17:37:53.322219: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_201', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-05-31 17:37:51.027547: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_201_0', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-05-31 17:37:51.085186: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_201', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-05-31 17:37:51.174874: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_201_0', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "pred_probs = model.predict(X_sub, batch_size=batch_size)\n",
    "pred_labels = np.argmax(pred_probs, axis=1)  # числа от 0 до 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103969fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'] = pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22b41cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a91257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Category\n",
       "0   0        19\n",
       "1   1         8\n",
       "2   2        14\n",
       "3   3        12\n",
       "4   4        10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('submission.csv')\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
